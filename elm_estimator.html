<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>UncELMe.elm_estimator API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>UncELMe.elm_estimator</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-

import numpy as np
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from .base import _Activation_function

class ELM(BaseEstimator, RegressorMixin):
    &#39;&#39;&#39;
    Instances of the ELM class are scikit-learn compatible estimators for regression 
    based on Extreme Learning Machine (ELM).

    &#39;&#39;&#39;
    
    def __init__(self, n_neurons=100, activation=&#39;logistic&#39;, 
                 weight_distr=&#39;uniform&#39;, weight_scl=1.0, random_state=None):
        &#39;&#39;&#39;
        Parameters
        ----------
        n_neurons : integer,
            Number of neurons. The default is 100.
        activation : string, optional
            Activation function (&#39;logistic&#39; or &#39;tanh&#39;). The default is &#39;logistic&#39;.
        weight_distr : string, optional
            Distribution of weights (&#39;uniform&#39; or &#39;gaussian&#39;). 
            The default is &#39;uniform&#39;. 
        weight_scl : float, optional
            Control the scale of the weight distribution.
            If weights are uniforms, they are drawn from [-weight_scl, weight_scl].
            If weights are Gaussians, they are centred with standard deviation
            equal to weight_scl. The default is 1.0.
        random_state : integer, optional
            Random seed for reproductible results. The default is None.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        
        self.n_neurons = n_neurons
        self.activation = activation
        self.weight_distr = weight_distr
        self.weight_scl = weight_scl
        self.random_state = random_state    
                
    def _H_compute(self, X):        
        &#39;&#39;&#39;
        Parameters
        ----------
        X : Numpy array of shape (n_samples, n_features)
            Input data.

        Returns
        -------
        H : Numpy array of shape (n_samples, n_neurons)
            Data projected in the random feature space.
            
        &#39;&#39;&#39;    
    
        check_is_fitted(self, [&#39;X_&#39;, &#39;y_&#39;])
        X = check_array(X)
        
        n_obs = X.shape[0]
        
        Input_weight = self.coef_hidden_
        Bias = np.reshape(self.intercept_hidden_, (1, -1))
        Bias_rep = Bias.repeat(n_obs, axis = 0)
        
        H = _Activation_function(X @ Input_weight + Bias_rep,
                                  func = self.activation)
        
        return H
    
    def _weight_draw(self):        
        &#39;&#39;&#39;
        Draw random input weights of the hidden layer.
        
        Parameters
        ----------
        None

        Returns
        -------
        None
            
        &#39;&#39;&#39;    
    
        n_feat = self.X_.shape[1]
        
        np.random.seed(self.random_state)
        
        if self.weight_distr == &#39;uniform&#39;:
            drawing = np.random.uniform(-self.weight_scl, self.weight_scl, 
                                        (n_feat+1, self.n_neurons))
        elif self.weight_distr == &#39;gaussian&#39;:
            drawing = np.random.normal(0, self.weight_scl, 
                                       (n_feat+1, self.n_neurons))
        else :
            raise TypeError(&#34;Only &#39;uniform&#39; and &#39;gaussian&#39; are available for the &#39;weight_distr&#39; argument&#34;)
        
        self.coef_hidden_ = drawing[:-1,:]   # random input weights, #neurons x #features, array
        self.intercept_hidden_ = drawing[-1,:] # random bias, #neurons, array
        
        return 
    
    def fit(self, X, y):
        &#39;&#39;&#39;
        Training for ELM.
        Initialize random hidden weights and compute output weights.
        
        Parameters
        ----------
        X : Numpy array of shape (n_sample_train, n_features)
            Training data.
        y : Numpy array of shape (n_sample_train)
            Target values.

        Returns
        -------
        Self.
        
        Reference
        ---------
        G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, 
        Extreme learning machine: theory and applications, 
        Neurocomputing 70 (1-3) (2006) 489–501.  
        
        &#39;&#39;&#39;
        
        X, y = check_X_y(X, y)
        
        self.X_ = X
        self.y_ = y
        
        n_obs, n_feat = X.shape
        
        self._weight_draw()
                
        self.H_ = self._H_compute(X)
        
        self.H_pinv_ = np.linalg.pinv(self.H_, rcond = np.sqrt(np.finfo(float).eps)) 
            
        y = y.T
        self.coef_output_ = (self.H_pinv_ @ y).squeeze() # neurons x #resp, array

        return self  
    
    def predict(self, X_predict):
        &#39;&#39;&#39;
        Parameters
        ----------
        X_predict : Numpy array of shape (n_samples, n_features)
            Input data to predict.

        Returns
        -------
        y_predict : Numpy array of shape (n_samples)
            Predicted output

        &#39;&#39;&#39; 

        H_predict = self._H_compute(X_predict)
        Output_weight = self.coef_output_.T
        
        y_predict = np.squeeze(H_predict @ Output_weight)
                
        return y_predict
    
    
class ELMRidge(ELM, BaseEstimator, RegressorMixin):
    &#39;&#39;&#39;
    Instances of the ELMRidge class are scikit-learn compatible estimators for regression 
    based on Extreme Learning Machine (ELM) with a regularization parameter (ridge estimate).
    
    &#39;&#39;&#39;
    
    def __init__(self, n_neurons=100, alpha=1.0, activation=&#39;logistic&#39;, 
                 weight_distr=&#39;uniform&#39;, weight_scl=1.0, random_state=None):
        &#39;&#39;&#39;
        Parameters
        ----------
        n_neurons : integer,
            Number of neurons. The default is 100.
        alpha : float, optional
            Regularization strength, aka Tikhonov factor. The default is 1.0.
        activation : string, optional
            Activation function (&#39;logistic&#39; or &#39;tanh&#39;). The default is &#39;logistic&#39;.
        weight_distr : string, optional
            Distribution of weights (&#39;uniform&#39; or &#39;gaussian&#39;). 
            The default is &#39;uniform&#39;. 
        weight_scl : float, optional
            Control the scale of the weight distribution.
            If weights are uniforms, they are drawn from [-weight_scl, weight_scl].
            If weights are Gaussians, they are centred with standard deviation
            equal to weight_scl. The default is 1.0.
        random_state : integer, optional
            Random seed for reproductible results. The default is None.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        
        ELM.__init__(self, n_neurons, activation, weight_distr, weight_scl, random_state)
        self.alpha = alpha                  
    
    def fit(self, X, y):
        &#39;&#39;&#39;
        Training for regularized ELM.
        Initialize random hidden weights and compute output weights.
        
        Parameters
        ----------
        X : Numpy array of shape (n_sample_train, n_features)
            Training data.
        y : Numpy array of shape (n_sample_train)
            Target values.

        Returns
        -------
        Self.
        
        Reference
        ---------
        W. Deng, Q. Zheng, L. Chen,  
        Regularized extreme learning machine, 
        IEEE symposium on computational intelligence and data mining, 2009, pp. 389–395.
        
        &#39;&#39;&#39;
        
        X, y = check_X_y(X, y)
        
        self.X_ = X
        self.y_ = y
        
        n_obs, n_feat = X.shape
        
        self._weight_draw()
                
        self.H_ = self._H_compute(X)   

        H_Tikh = self.H_.T @ self.H_ + self.alpha * np.identity(self.n_neurons)
        self.H_alpha_ = np.linalg.pinv(H_Tikh) @ self.H_.T
            
        y = y.T
        self.coef_output_ = (self.H_alpha_ @ y).squeeze() # neurons x #resp, array

        return self
    

class ELMRidgeCV(ELM, BaseEstimator, RegressorMixin):
    &#39;&#39;&#39;
    Instances of the ELMRidgeCV class are scikit-learn compatible estimators for regression 
    based on Extreme Learning Machine (ELM) with a regularization parameter (ridge estimate).
    The regularization parameter is optimized by Generalized Cross Validation (GCV).
    
    &#39;&#39;&#39;    

    def __init__(self, n_neurons=100, alphas=np.array([0.1, 1.0, 10]), activation=&#39;logistic&#39;, 
                 weight_distr=&#39;uniform&#39;, weight_scl=1.0, random_state=None):
        &#39;&#39;&#39;
        Parameters
        ----------
        n_neurons : integer,
            Number of neurons. The default is 100.
        alphas : ndarray, optional
            Array of alpha&#39;s values to try. Regularization strength, 
            aka Tikhonov factor. The default is np.array([0.1, 1.0, 10]).
        activation : string, optional
            Activation function (&#39;logistic&#39; or &#39;tanh&#39;). The default is &#39;logistic&#39;.
        weight_distr : string, optional
            Distribution of weights (&#39;uniform&#39; or &#39;gaussian&#39;). 
            The default is &#39;uniform&#39;. 
        weight_scl : float, optional
            Controle the scale of the weight distribution.
            If weights are uniforms, they are drawn from [-weight_scl, weight_scl].
            If weights are Gaussians, they are centred with standard deviation
            equal to weight_scl. The default is 1.0.
        random_state : integer, optional
            Random seed for reproductible results. The default is None.

        Returns
        -------
        None.
        
        &#39;&#39;&#39;
        
        ELM.__init__(self, n_neurons, activation, weight_distr, weight_scl, random_state)
        self.alphas = alphas             
    
    def fit(self, X, y):
        &#39;&#39;&#39;
        Training for ridge ELM, with Generalized Cross Validation.
        Initialize random hidden weights and compute output weights.
        
        Parameters
        ----------
        X : Numpy array of shape (n_sample_train, n_features)
            Training data.
        y : Numpy array of shape (n_sample_train)
            Target values.

        Returns
        -------
        Self.
        
        References
        ----------
        W. Deng, Q. Zheng, L. Chen,  
        Regularized extreme learning machine, 
        IEEE symposium on computational intelligence and data mining, 2009, pp. 389–395.
        
        G. H. Golub, M. Heath, G. Wahba, 
        Generalized cross-validation as a method for choosing a good ridge parameter,
        Technometrics 21 (2) (1979) 215–223.
        
        &#39;&#39;&#39;
        
        X, y = check_X_y(X, y)
        
        self.X_ = X
        self.y_ = y
        
        n_obs, n_feat = X.shape
        
        self._weight_draw()
                
        self.H_ = self._H_compute(X)
        
        eigenHTH = np.square(np.linalg.svd(self.H_ , full_matrices= False, compute_uv=False, hermitian=False))
        eigenHTH = eigenHTH.reshape(eigenHTH.shape[0], 1)
        trace = (eigenHTH/(eigenHTH + self.alphas)).sum(axis=0)      
        
        HTH = self.H_.T@self.H_
        H_Tikh = HTH + self.alphas.reshape(self.alphas.shape[0], 1, 1) * np.identity(self.n_neurons)
        H_Tikh_inv = np.linalg.pinv(H_Tikh)
        
        y_hat = np.einsum(&#39;li, aij, kj, k -&gt; al&#39;, self.H_, H_Tikh_inv, 
                          self.H_, y, optimize = &#39;greedy&#39;)
        self.GCV = np.linalg.norm(y-y_hat, axis = 1)
        self.GCV = n_obs * self.GCV /np.square(n_obs-trace)
        self.alpha_opt = self.alphas[np.argmin(self.GCV)]
        self.H_alpha_ = H_Tikh_inv[np.argmin(self.GCV)] @ self.H_.T
                        
        y = y.T
        self.coef_output_ = (self.H_alpha_ @ y).squeeze() # neurons x #resp, array

        return self</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="UncELMe.elm_estimator.ELM"><code class="flex name class">
<span>class <span class="ident">ELM</span></span>
<span>(</span><span>n_neurons=100, activation='logistic', weight_distr='uniform', weight_scl=1.0, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Instances of the ELM class are scikit-learn compatible estimators for regression
based on Extreme Learning Machine (ELM).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_neurons</code></strong> :&ensp;<code>integer,</code></dt>
<dd>Number of neurons. The default is 100.</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Activation function ('logistic' or 'tanh'). The default is 'logistic'.</dd>
<dt><strong><code>weight_distr</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Distribution of weights ('uniform' or 'gaussian').
The default is 'uniform'.</dd>
<dt><strong><code>weight_scl</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Control the scale of the weight distribution.
If weights are uniforms, they are drawn from [-weight_scl, weight_scl].
If weights are Gaussians, they are centred with standard deviation
equal to weight_scl. The default is 1.0.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>integer</code>, optional</dt>
<dd>Random seed for reproductible results. The default is None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ELM(BaseEstimator, RegressorMixin):
    &#39;&#39;&#39;
    Instances of the ELM class are scikit-learn compatible estimators for regression 
    based on Extreme Learning Machine (ELM).

    &#39;&#39;&#39;
    
    def __init__(self, n_neurons=100, activation=&#39;logistic&#39;, 
                 weight_distr=&#39;uniform&#39;, weight_scl=1.0, random_state=None):
        &#39;&#39;&#39;
        Parameters
        ----------
        n_neurons : integer,
            Number of neurons. The default is 100.
        activation : string, optional
            Activation function (&#39;logistic&#39; or &#39;tanh&#39;). The default is &#39;logistic&#39;.
        weight_distr : string, optional
            Distribution of weights (&#39;uniform&#39; or &#39;gaussian&#39;). 
            The default is &#39;uniform&#39;. 
        weight_scl : float, optional
            Control the scale of the weight distribution.
            If weights are uniforms, they are drawn from [-weight_scl, weight_scl].
            If weights are Gaussians, they are centred with standard deviation
            equal to weight_scl. The default is 1.0.
        random_state : integer, optional
            Random seed for reproductible results. The default is None.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        
        self.n_neurons = n_neurons
        self.activation = activation
        self.weight_distr = weight_distr
        self.weight_scl = weight_scl
        self.random_state = random_state    
                
    def _H_compute(self, X):        
        &#39;&#39;&#39;
        Parameters
        ----------
        X : Numpy array of shape (n_samples, n_features)
            Input data.

        Returns
        -------
        H : Numpy array of shape (n_samples, n_neurons)
            Data projected in the random feature space.
            
        &#39;&#39;&#39;    
    
        check_is_fitted(self, [&#39;X_&#39;, &#39;y_&#39;])
        X = check_array(X)
        
        n_obs = X.shape[0]
        
        Input_weight = self.coef_hidden_
        Bias = np.reshape(self.intercept_hidden_, (1, -1))
        Bias_rep = Bias.repeat(n_obs, axis = 0)
        
        H = _Activation_function(X @ Input_weight + Bias_rep,
                                  func = self.activation)
        
        return H
    
    def _weight_draw(self):        
        &#39;&#39;&#39;
        Draw random input weights of the hidden layer.
        
        Parameters
        ----------
        None

        Returns
        -------
        None
            
        &#39;&#39;&#39;    
    
        n_feat = self.X_.shape[1]
        
        np.random.seed(self.random_state)
        
        if self.weight_distr == &#39;uniform&#39;:
            drawing = np.random.uniform(-self.weight_scl, self.weight_scl, 
                                        (n_feat+1, self.n_neurons))
        elif self.weight_distr == &#39;gaussian&#39;:
            drawing = np.random.normal(0, self.weight_scl, 
                                       (n_feat+1, self.n_neurons))
        else :
            raise TypeError(&#34;Only &#39;uniform&#39; and &#39;gaussian&#39; are available for the &#39;weight_distr&#39; argument&#34;)
        
        self.coef_hidden_ = drawing[:-1,:]   # random input weights, #neurons x #features, array
        self.intercept_hidden_ = drawing[-1,:] # random bias, #neurons, array
        
        return 
    
    def fit(self, X, y):
        &#39;&#39;&#39;
        Training for ELM.
        Initialize random hidden weights and compute output weights.
        
        Parameters
        ----------
        X : Numpy array of shape (n_sample_train, n_features)
            Training data.
        y : Numpy array of shape (n_sample_train)
            Target values.

        Returns
        -------
        Self.
        
        Reference
        ---------
        G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, 
        Extreme learning machine: theory and applications, 
        Neurocomputing 70 (1-3) (2006) 489–501.  
        
        &#39;&#39;&#39;
        
        X, y = check_X_y(X, y)
        
        self.X_ = X
        self.y_ = y
        
        n_obs, n_feat = X.shape
        
        self._weight_draw()
                
        self.H_ = self._H_compute(X)
        
        self.H_pinv_ = np.linalg.pinv(self.H_, rcond = np.sqrt(np.finfo(float).eps)) 
            
        y = y.T
        self.coef_output_ = (self.H_pinv_ @ y).squeeze() # neurons x #resp, array

        return self  
    
    def predict(self, X_predict):
        &#39;&#39;&#39;
        Parameters
        ----------
        X_predict : Numpy array of shape (n_samples, n_features)
            Input data to predict.

        Returns
        -------
        y_predict : Numpy array of shape (n_samples)
            Predicted output

        &#39;&#39;&#39; 

        H_predict = self._H_compute(X_predict)
        Output_weight = self.coef_output_.T
        
        y_predict = np.squeeze(H_predict @ Output_weight)
                
        return y_predict</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.RegressorMixin</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="UncELMe.elm_estimator.ELMRidge" href="#UncELMe.elm_estimator.ELMRidge">ELMRidge</a></li>
<li><a title="UncELMe.elm_estimator.ELMRidgeCV" href="#UncELMe.elm_estimator.ELMRidgeCV">ELMRidgeCV</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="UncELMe.elm_estimator.ELM.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Training for ELM.
Initialize random hidden weights and compute output weights.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>Numpy array</code> of <code>shape (n_sample_train, n_features)</code></dt>
<dd>Training data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>Numpy array</code> of <code>shape (n_sample_train)</code></dt>
<dd>Target values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Self.</p>
<h2 id="reference">Reference</h2>
<p>G.-B. Huang, Q.-Y. Zhu, C.-K. Siew,
Extreme learning machine: theory and applications,
Neurocomputing 70 (1-3) (2006) 489–501.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y):
    &#39;&#39;&#39;
    Training for ELM.
    Initialize random hidden weights and compute output weights.
    
    Parameters
    ----------
    X : Numpy array of shape (n_sample_train, n_features)
        Training data.
    y : Numpy array of shape (n_sample_train)
        Target values.

    Returns
    -------
    Self.
    
    Reference
    ---------
    G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, 
    Extreme learning machine: theory and applications, 
    Neurocomputing 70 (1-3) (2006) 489–501.  
    
    &#39;&#39;&#39;
    
    X, y = check_X_y(X, y)
    
    self.X_ = X
    self.y_ = y
    
    n_obs, n_feat = X.shape
    
    self._weight_draw()
            
    self.H_ = self._H_compute(X)
    
    self.H_pinv_ = np.linalg.pinv(self.H_, rcond = np.sqrt(np.finfo(float).eps)) 
        
    y = y.T
    self.coef_output_ = (self.H_pinv_ @ y).squeeze() # neurons x #resp, array

    return self  </code></pre>
</details>
</dd>
<dt id="UncELMe.elm_estimator.ELM.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X_predict)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X_predict</code></strong> :&ensp;<code>Numpy array</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Input data to predict.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y_predict</code></strong> :&ensp;<code>Numpy array</code> of <code>shape (n_samples)</code></dt>
<dd>Predicted output</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X_predict):
    &#39;&#39;&#39;
    Parameters
    ----------
    X_predict : Numpy array of shape (n_samples, n_features)
        Input data to predict.

    Returns
    -------
    y_predict : Numpy array of shape (n_samples)
        Predicted output

    &#39;&#39;&#39; 

    H_predict = self._H_compute(X_predict)
    Output_weight = self.coef_output_.T
    
    y_predict = np.squeeze(H_predict @ Output_weight)
            
    return y_predict</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="UncELMe.elm_estimator.ELMRidge"><code class="flex name class">
<span>class <span class="ident">ELMRidge</span></span>
<span>(</span><span>n_neurons=100, alpha=1.0, activation='logistic', weight_distr='uniform', weight_scl=1.0, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Instances of the ELMRidge class are scikit-learn compatible estimators for regression
based on Extreme Learning Machine (ELM) with a regularization parameter (ridge estimate).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_neurons</code></strong> :&ensp;<code>integer,</code></dt>
<dd>Number of neurons. The default is 100.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Regularization strength, aka Tikhonov factor. The default is 1.0.</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Activation function ('logistic' or 'tanh'). The default is 'logistic'.</dd>
<dt><strong><code>weight_distr</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Distribution of weights ('uniform' or 'gaussian').
The default is 'uniform'.</dd>
<dt><strong><code>weight_scl</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Control the scale of the weight distribution.
If weights are uniforms, they are drawn from [-weight_scl, weight_scl].
If weights are Gaussians, they are centred with standard deviation
equal to weight_scl. The default is 1.0.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>integer</code>, optional</dt>
<dd>Random seed for reproductible results. The default is None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ELMRidge(ELM, BaseEstimator, RegressorMixin):
    &#39;&#39;&#39;
    Instances of the ELMRidge class are scikit-learn compatible estimators for regression 
    based on Extreme Learning Machine (ELM) with a regularization parameter (ridge estimate).
    
    &#39;&#39;&#39;
    
    def __init__(self, n_neurons=100, alpha=1.0, activation=&#39;logistic&#39;, 
                 weight_distr=&#39;uniform&#39;, weight_scl=1.0, random_state=None):
        &#39;&#39;&#39;
        Parameters
        ----------
        n_neurons : integer,
            Number of neurons. The default is 100.
        alpha : float, optional
            Regularization strength, aka Tikhonov factor. The default is 1.0.
        activation : string, optional
            Activation function (&#39;logistic&#39; or &#39;tanh&#39;). The default is &#39;logistic&#39;.
        weight_distr : string, optional
            Distribution of weights (&#39;uniform&#39; or &#39;gaussian&#39;). 
            The default is &#39;uniform&#39;. 
        weight_scl : float, optional
            Control the scale of the weight distribution.
            If weights are uniforms, they are drawn from [-weight_scl, weight_scl].
            If weights are Gaussians, they are centred with standard deviation
            equal to weight_scl. The default is 1.0.
        random_state : integer, optional
            Random seed for reproductible results. The default is None.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        
        ELM.__init__(self, n_neurons, activation, weight_distr, weight_scl, random_state)
        self.alpha = alpha                  
    
    def fit(self, X, y):
        &#39;&#39;&#39;
        Training for regularized ELM.
        Initialize random hidden weights and compute output weights.
        
        Parameters
        ----------
        X : Numpy array of shape (n_sample_train, n_features)
            Training data.
        y : Numpy array of shape (n_sample_train)
            Target values.

        Returns
        -------
        Self.
        
        Reference
        ---------
        W. Deng, Q. Zheng, L. Chen,  
        Regularized extreme learning machine, 
        IEEE symposium on computational intelligence and data mining, 2009, pp. 389–395.
        
        &#39;&#39;&#39;
        
        X, y = check_X_y(X, y)
        
        self.X_ = X
        self.y_ = y
        
        n_obs, n_feat = X.shape
        
        self._weight_draw()
                
        self.H_ = self._H_compute(X)   

        H_Tikh = self.H_.T @ self.H_ + self.alpha * np.identity(self.n_neurons)
        self.H_alpha_ = np.linalg.pinv(H_Tikh) @ self.H_.T
            
        y = y.T
        self.coef_output_ = (self.H_alpha_ @ y).squeeze() # neurons x #resp, array

        return self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="UncELMe.elm_estimator.ELM" href="#UncELMe.elm_estimator.ELM">ELM</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.RegressorMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="UncELMe.elm_estimator.ELMRidge.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Training for regularized ELM.
Initialize random hidden weights and compute output weights.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>Numpy array</code> of <code>shape (n_sample_train, n_features)</code></dt>
<dd>Training data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>Numpy array</code> of <code>shape (n_sample_train)</code></dt>
<dd>Target values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Self.</p>
<h2 id="reference">Reference</h2>
<p>W. Deng, Q. Zheng, L. Chen,<br>
Regularized extreme learning machine,
IEEE symposium on computational intelligence and data mining, 2009, pp. 389–395.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y):
    &#39;&#39;&#39;
    Training for regularized ELM.
    Initialize random hidden weights and compute output weights.
    
    Parameters
    ----------
    X : Numpy array of shape (n_sample_train, n_features)
        Training data.
    y : Numpy array of shape (n_sample_train)
        Target values.

    Returns
    -------
    Self.
    
    Reference
    ---------
    W. Deng, Q. Zheng, L. Chen,  
    Regularized extreme learning machine, 
    IEEE symposium on computational intelligence and data mining, 2009, pp. 389–395.
    
    &#39;&#39;&#39;
    
    X, y = check_X_y(X, y)
    
    self.X_ = X
    self.y_ = y
    
    n_obs, n_feat = X.shape
    
    self._weight_draw()
            
    self.H_ = self._H_compute(X)   

    H_Tikh = self.H_.T @ self.H_ + self.alpha * np.identity(self.n_neurons)
    self.H_alpha_ = np.linalg.pinv(H_Tikh) @ self.H_.T
        
    y = y.T
    self.coef_output_ = (self.H_alpha_ @ y).squeeze() # neurons x #resp, array

    return self</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="UncELMe.elm_estimator.ELM" href="#UncELMe.elm_estimator.ELM">ELM</a></b></code>:
<ul class="hlist">
<li><code><a title="UncELMe.elm_estimator.ELM.predict" href="#UncELMe.elm_estimator.ELM.predict">predict</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="UncELMe.elm_estimator.ELMRidgeCV"><code class="flex name class">
<span>class <span class="ident">ELMRidgeCV</span></span>
<span>(</span><span>n_neurons=100, alphas=array([ 0.1,
1. , 10. ]), activation='logistic', weight_distr='uniform', weight_scl=1.0, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Instances of the ELMRidgeCV class are scikit-learn compatible estimators for regression
based on Extreme Learning Machine (ELM) with a regularization parameter (ridge estimate).
The regularization parameter is optimized by Generalized Cross Validation (GCV).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_neurons</code></strong> :&ensp;<code>integer,</code></dt>
<dd>Number of neurons. The default is 100.</dd>
<dt><strong><code>alphas</code></strong> :&ensp;<code>ndarray</code>, optional</dt>
<dd>Array of alpha's values to try. Regularization strength,
aka Tikhonov factor. The default is np.array([0.1, 1.0, 10]).</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Activation function ('logistic' or 'tanh'). The default is 'logistic'.</dd>
<dt><strong><code>weight_distr</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Distribution of weights ('uniform' or 'gaussian').
The default is 'uniform'.</dd>
<dt><strong><code>weight_scl</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Controle the scale of the weight distribution.
If weights are uniforms, they are drawn from [-weight_scl, weight_scl].
If weights are Gaussians, they are centred with standard deviation
equal to weight_scl. The default is 1.0.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>integer</code>, optional</dt>
<dd>Random seed for reproductible results. The default is None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ELMRidgeCV(ELM, BaseEstimator, RegressorMixin):
    &#39;&#39;&#39;
    Instances of the ELMRidgeCV class are scikit-learn compatible estimators for regression 
    based on Extreme Learning Machine (ELM) with a regularization parameter (ridge estimate).
    The regularization parameter is optimized by Generalized Cross Validation (GCV).
    
    &#39;&#39;&#39;    

    def __init__(self, n_neurons=100, alphas=np.array([0.1, 1.0, 10]), activation=&#39;logistic&#39;, 
                 weight_distr=&#39;uniform&#39;, weight_scl=1.0, random_state=None):
        &#39;&#39;&#39;
        Parameters
        ----------
        n_neurons : integer,
            Number of neurons. The default is 100.
        alphas : ndarray, optional
            Array of alpha&#39;s values to try. Regularization strength, 
            aka Tikhonov factor. The default is np.array([0.1, 1.0, 10]).
        activation : string, optional
            Activation function (&#39;logistic&#39; or &#39;tanh&#39;). The default is &#39;logistic&#39;.
        weight_distr : string, optional
            Distribution of weights (&#39;uniform&#39; or &#39;gaussian&#39;). 
            The default is &#39;uniform&#39;. 
        weight_scl : float, optional
            Controle the scale of the weight distribution.
            If weights are uniforms, they are drawn from [-weight_scl, weight_scl].
            If weights are Gaussians, they are centred with standard deviation
            equal to weight_scl. The default is 1.0.
        random_state : integer, optional
            Random seed for reproductible results. The default is None.

        Returns
        -------
        None.
        
        &#39;&#39;&#39;
        
        ELM.__init__(self, n_neurons, activation, weight_distr, weight_scl, random_state)
        self.alphas = alphas             
    
    def fit(self, X, y):
        &#39;&#39;&#39;
        Training for ridge ELM, with Generalized Cross Validation.
        Initialize random hidden weights and compute output weights.
        
        Parameters
        ----------
        X : Numpy array of shape (n_sample_train, n_features)
            Training data.
        y : Numpy array of shape (n_sample_train)
            Target values.

        Returns
        -------
        Self.
        
        References
        ----------
        W. Deng, Q. Zheng, L. Chen,  
        Regularized extreme learning machine, 
        IEEE symposium on computational intelligence and data mining, 2009, pp. 389–395.
        
        G. H. Golub, M. Heath, G. Wahba, 
        Generalized cross-validation as a method for choosing a good ridge parameter,
        Technometrics 21 (2) (1979) 215–223.
        
        &#39;&#39;&#39;
        
        X, y = check_X_y(X, y)
        
        self.X_ = X
        self.y_ = y
        
        n_obs, n_feat = X.shape
        
        self._weight_draw()
                
        self.H_ = self._H_compute(X)
        
        eigenHTH = np.square(np.linalg.svd(self.H_ , full_matrices= False, compute_uv=False, hermitian=False))
        eigenHTH = eigenHTH.reshape(eigenHTH.shape[0], 1)
        trace = (eigenHTH/(eigenHTH + self.alphas)).sum(axis=0)      
        
        HTH = self.H_.T@self.H_
        H_Tikh = HTH + self.alphas.reshape(self.alphas.shape[0], 1, 1) * np.identity(self.n_neurons)
        H_Tikh_inv = np.linalg.pinv(H_Tikh)
        
        y_hat = np.einsum(&#39;li, aij, kj, k -&gt; al&#39;, self.H_, H_Tikh_inv, 
                          self.H_, y, optimize = &#39;greedy&#39;)
        self.GCV = np.linalg.norm(y-y_hat, axis = 1)
        self.GCV = n_obs * self.GCV /np.square(n_obs-trace)
        self.alpha_opt = self.alphas[np.argmin(self.GCV)]
        self.H_alpha_ = H_Tikh_inv[np.argmin(self.GCV)] @ self.H_.T
                        
        y = y.T
        self.coef_output_ = (self.H_alpha_ @ y).squeeze() # neurons x #resp, array

        return self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="UncELMe.elm_estimator.ELM" href="#UncELMe.elm_estimator.ELM">ELM</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.RegressorMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="UncELMe.elm_estimator.ELMRidgeCV.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Training for ridge ELM, with Generalized Cross Validation.
Initialize random hidden weights and compute output weights.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>Numpy array</code> of <code>shape (n_sample_train, n_features)</code></dt>
<dd>Training data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>Numpy array</code> of <code>shape (n_sample_train)</code></dt>
<dd>Target values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Self.</p>
<h2 id="references">References</h2>
<p>W. Deng, Q. Zheng, L. Chen,<br>
Regularized extreme learning machine,
IEEE symposium on computational intelligence and data mining, 2009, pp. 389–395.</p>
<p>G. H. Golub, M. Heath, G. Wahba,
Generalized cross-validation as a method for choosing a good ridge parameter,
Technometrics 21 (2) (1979) 215–223.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y):
    &#39;&#39;&#39;
    Training for ridge ELM, with Generalized Cross Validation.
    Initialize random hidden weights and compute output weights.
    
    Parameters
    ----------
    X : Numpy array of shape (n_sample_train, n_features)
        Training data.
    y : Numpy array of shape (n_sample_train)
        Target values.

    Returns
    -------
    Self.
    
    References
    ----------
    W. Deng, Q. Zheng, L. Chen,  
    Regularized extreme learning machine, 
    IEEE symposium on computational intelligence and data mining, 2009, pp. 389–395.
    
    G. H. Golub, M. Heath, G. Wahba, 
    Generalized cross-validation as a method for choosing a good ridge parameter,
    Technometrics 21 (2) (1979) 215–223.
    
    &#39;&#39;&#39;
    
    X, y = check_X_y(X, y)
    
    self.X_ = X
    self.y_ = y
    
    n_obs, n_feat = X.shape
    
    self._weight_draw()
            
    self.H_ = self._H_compute(X)
    
    eigenHTH = np.square(np.linalg.svd(self.H_ , full_matrices= False, compute_uv=False, hermitian=False))
    eigenHTH = eigenHTH.reshape(eigenHTH.shape[0], 1)
    trace = (eigenHTH/(eigenHTH + self.alphas)).sum(axis=0)      
    
    HTH = self.H_.T@self.H_
    H_Tikh = HTH + self.alphas.reshape(self.alphas.shape[0], 1, 1) * np.identity(self.n_neurons)
    H_Tikh_inv = np.linalg.pinv(H_Tikh)
    
    y_hat = np.einsum(&#39;li, aij, kj, k -&gt; al&#39;, self.H_, H_Tikh_inv, 
                      self.H_, y, optimize = &#39;greedy&#39;)
    self.GCV = np.linalg.norm(y-y_hat, axis = 1)
    self.GCV = n_obs * self.GCV /np.square(n_obs-trace)
    self.alpha_opt = self.alphas[np.argmin(self.GCV)]
    self.H_alpha_ = H_Tikh_inv[np.argmin(self.GCV)] @ self.H_.T
                    
    y = y.T
    self.coef_output_ = (self.H_alpha_ @ y).squeeze() # neurons x #resp, array

    return self</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="UncELMe.elm_estimator.ELM" href="#UncELMe.elm_estimator.ELM">ELM</a></b></code>:
<ul class="hlist">
<li><code><a title="UncELMe.elm_estimator.ELM.predict" href="#UncELMe.elm_estimator.ELM.predict">predict</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="UncELMe" href="index.html">UncELMe</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="UncELMe.elm_estimator.ELM" href="#UncELMe.elm_estimator.ELM">ELM</a></code></h4>
<ul class="">
<li><code><a title="UncELMe.elm_estimator.ELM.fit" href="#UncELMe.elm_estimator.ELM.fit">fit</a></code></li>
<li><code><a title="UncELMe.elm_estimator.ELM.predict" href="#UncELMe.elm_estimator.ELM.predict">predict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="UncELMe.elm_estimator.ELMRidge" href="#UncELMe.elm_estimator.ELMRidge">ELMRidge</a></code></h4>
<ul class="">
<li><code><a title="UncELMe.elm_estimator.ELMRidge.fit" href="#UncELMe.elm_estimator.ELMRidge.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="UncELMe.elm_estimator.ELMRidgeCV" href="#UncELMe.elm_estimator.ELMRidgeCV">ELMRidgeCV</a></code></h4>
<ul class="">
<li><code><a title="UncELMe.elm_estimator.ELMRidgeCV.fit" href="#UncELMe.elm_estimator.ELMRidgeCV.fit">fit</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>